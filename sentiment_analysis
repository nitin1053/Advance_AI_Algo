# Install required packages (if not already installed)
!pip install nltk textblob

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from textblob import TextBlob
import matplotlib.pyplot as plt

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load data from Text3.txt
with open("/content/Text 3.txt", "r") as file:
    texts = file.readlines()

# Preprocessing function
def preprocess_text(text):
    # Tokenize text
    tokens = word_tokenize(text.lower())
    
    # Lemmatization and stop-word removal
    lemmatizer = nltk.WordNetLemmatizer()
    processed_tokens = [
        lemmatizer.lemmatize(token)
        for token in tokens
        if token not in stopwords.words('english') and token.isalpha()
    ]
    return ' '.join(processed_tokens)

# Preprocess each text
preprocessed_texts = [preprocess_text(text) for text in texts]

# Function to analyze sentiment
def get_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

# Calculate sentiment scores
sentiments = [get_sentiment(text) for text in preprocessed_texts]

# Print results
for original_text, sentiment in zip(texts, sentiments):
    print(f"Text: {original_text.strip()}\nSentiment Score: {sentiment}\n")

# Plot sentiment scores
plt.bar(range(len(sentiments)), sentiments)
plt.xlabel("Text Index")
plt.ylabel("Sentiment Score")
plt.title("Sentiment Analysis of Texts from Text3.txt")
plt.show()
